from fastmcp import FastMCP
import os
import sys
import warnings
import traceback

# ---------------------------------------------------------
# ä¿®æ”¹é» 1: ç§»é™¤é ‚å±¤çš„ Heavy Imports
# åªä¿ç•™ os, sys, fastmcp ç­‰è¼•é‡ç´šå¥—ä»¶
# é€™æ¨£ Server å•Ÿå‹•æ™‚å°±ä¸æœƒå¡åœ¨è¼‰å…¥ PyTorch/LangChain
# ---------------------------------------------------------

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore")

mcp = FastMCP("My Local Library")

def get_base_path():
    """
    åˆ¤æ–·åŸ·è¡Œç’°å¢ƒï¼š
    å¦‚æœæ˜¯æ‰“åŒ…å¾Œçš„ exeï¼Œä½¿ç”¨åŸ·è¡Œæª”æ‰€åœ¨çš„è³‡æ–™å¤¾ (sys.executable)
    å¦‚æœæ˜¯åŸå§‹ python scriptï¼Œä½¿ç”¨æª”æ¡ˆæ‰€åœ¨çš„è³‡æ–™å¤¾ (__file__)
    """
    if getattr(sys, 'frozen', False):
        # å¦‚æœæ˜¯æ‰“åŒ…å¾Œçš„ exe
        return os.path.dirname(sys.executable)
    else:
        # å¦‚æœæ˜¯é–‹ç™¼ä¸­çš„ .py
        return os.path.dirname(os.path.abspath(__file__))

BASE_DIR = get_base_path()
DB_DIR = os.path.join(BASE_DIR, "faiss_index")

# ---------------------------------------------------------
# ä¿®æ”¹é» 2: ä½¿ç”¨å…¨åŸŸè®Šæ•¸é…åˆ Singleton æ¨¡å¼å»¶é²è¼‰å…¥
# ---------------------------------------------------------
_embedding_function = None

def get_embedding_function():
    """
    å»¶é²è¼‰å…¥ Embedding æ¨¡å‹ã€‚
    åªæœ‰åœ¨ç¬¬ä¸€æ¬¡å‘¼å«å·¥å…·æ™‚æ‰æœƒåŸ·è¡Œ import å’Œæ¨¡å‹ä¸‹è¼‰ã€‚
    """
    global _embedding_function
    if _embedding_function is None:
        # å°‡ import ç§»åˆ°é€™è£¡ï¼Œé¿å…å•Ÿå‹•æ™‚å¡ä½
        from langchain_huggingface import HuggingFaceEmbeddings
        
        print("æ­£åœ¨è¼‰å…¥ AI æ¨¡å‹ (ç¬¬ä¸€æ¬¡åŸ·è¡Œæœƒè¼ƒæ…¢)...", file=sys.stderr)
        _embedding_function = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    return _embedding_function

def get_db():
    """çµ±ä¸€å–å¾— DB å¯¦ä¾‹ (åŒ…å«å»¶é²è¼‰å…¥ FAISS)"""
    # å°‡ import ç§»åˆ°é€™è£¡
    from langchain_community.vectorstores import FAISS
    
    # ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥
    embedding_func = get_embedding_function()
    
    if os.path.exists(DB_DIR) and os.path.exists(os.path.join(DB_DIR, "index.faiss")):
        return FAISS.load_local(DB_DIR, embedding_func, allow_dangerous_deserialization=True)
    return None

def save_db(db):
    """å°‡ç´¢å¼•å­˜å›ç¡¬ç¢Ÿ"""
    db.save_local(DB_DIR)

# ---------------------------------------------------------
# ä¿®æ”¹é» 3: å°‡å·¥å…·æ‰€éœ€çš„ Import ç§»è‡³å‡½å¼å…§éƒ¨
# ---------------------------------------------------------

@mcp.tool()
def add_folder_to_library(folder_path: str):
    """[æ‰¹æ¬¡è™•ç†] è®€å–è³‡æ–™å¤¾å…§ PDF ä¸¦åŠ å…¥çŸ¥è­˜åº«"""
    # Import moved locally
    import glob
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS

    folder_path = folder_path.strip('"').strip("'")
    
    if not os.path.exists(folder_path):
        return f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è³‡æ–™å¤¾ -> {folder_path}"

    pdf_files = glob.glob(os.path.join(folder_path, "*.pdf"))
    if not pdf_files:
        return f"åœ¨ '{folder_path}' ä¸­æ‰¾ä¸åˆ°ä»»ä½• PDF æª”æ¡ˆã€‚"

    all_splits = []
    processed_files = []
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)

    # 1. è®€å–ä¸¦åˆ‡åˆ†
    for pdf_file in pdf_files:
        try:
            loader = PyPDFLoader(pdf_file)
            docs = loader.load()
            splits = text_splitter.split_documents(docs)
            if splits:
                for split in splits:
                    split.metadata["source"] = os.path.basename(pdf_file)
                all_splits.extend(splits)
                processed_files.append(os.path.basename(pdf_file))
        except Exception as e:
            print(f"Error reading {pdf_file}: {e}", file=sys.stderr)

    if not all_splits:
        return "æ²’æœ‰æˆåŠŸè®€å–åˆ°ä»»ä½•å…§å®¹ã€‚"

    # 2. å¯«å…¥ FAISS
    try:
        current_db = get_db()
        embedding_func = get_embedding_function() # ç¢ºä¿å–å¾—æ¨¡å‹
        
        if current_db:
            current_db.add_documents(all_splits)
            save_db(current_db)
        else:
            # éœ€è¦ç”¨åˆ° FAISS classï¼Œæ‰€ä»¥ä¸Šé¢æœ‰ import
            new_db = FAISS.from_documents(all_splits, embedding_func)
            save_db(new_db)
        
        return f"æ‰¹æ¬¡è™•ç†å®Œæˆï¼å…±è™•ç† {len(processed_files)} å€‹æª”æ¡ˆï¼Œæ–°å¢ {len(all_splits)} å€‹ç‰‡æ®µã€‚"
    except Exception as e:
        return f"å¯«å…¥è³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"

@mcp.tool()
def add_pdf_to_library(pdf_path: str):
    """[å–®æª”è™•ç†] å°‡ PDF åŠ å…¥çŸ¥è­˜åº«"""
    # Import moved locally
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS

    pdf_path = pdf_path.strip('"').strip("'")
    
    if not os.path.exists(pdf_path):
        return f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ -> {pdf_path}"

    try:
        loader = PyPDFLoader(pdf_path)
        docs = loader.load()
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)
        splits = text_splitter.split_documents(docs)
        
        if not splits:
            return "PDF å…§å®¹ç‚ºç©ºæˆ–ç„¡æ³•è®€å–ã€‚"

        # å¯«å…¥ FAISS
        current_db = get_db()
        embedding_func = get_embedding_function()

        if current_db:
            current_db.add_documents(splits)
            save_db(current_db)
        else:
            new_db = FAISS.from_documents(splits, embedding_func)
            save_db(new_db)
        
        return f"æˆåŠŸï¼å·²å°‡ '{os.path.basename(pdf_path)}' çš„ {len(splits)} å€‹ç‰‡æ®µåŠ å…¥çŸ¥è­˜åº«ã€‚"
    
    except Exception as e:
        return f"è™•ç† PDF æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"

@mcp.tool()
def list_indexed_files():
    """[æŸ¥è©¢] åˆ—å‡ºçŸ¥è­˜åº«ä¸­å·²ç´¢å¼•çš„æ‰€æœ‰æ–‡ä»¶åŠçµ±è¨ˆè³‡è¨Š"""
    try:
        db = get_db()
        if not db:
            return "ğŸ“­ çŸ¥è­˜åº«ç›®å‰æ˜¯ç©ºçš„ï¼Œè«‹å…ˆä½¿ç”¨ add_pdf_to_library æˆ– add_folder_to_library åŠ å…¥æ–‡ä»¶ã€‚"

        # å¾ docstore ä¸­æå–æ‰€æœ‰æ–‡ä»¶çš„ metadata
        sources_info = {}  # source -> {'pages': set(), 'chunks': count}
        total_chunks = 0
        
        for doc_id in db.docstore._dict:
            doc = db.docstore._dict[doc_id]
            source = doc.metadata.get('source', 'æœªçŸ¥ä¾†æº')
            page = doc.metadata.get('page', None)
            
            if source not in sources_info:
                sources_info[source] = {'pages': set(), 'chunks': 0}
            
            sources_info[source]['chunks'] += 1
            if page is not None:
                sources_info[source]['pages'].add(page)
            total_chunks += 1

        # çµ„åˆè¼¸å‡ºçµæœ
        file_count = len(sources_info)
        result = f"ğŸ“š çŸ¥è­˜åº«çµ±è¨ˆè³‡è¨Š\n"
        result += f"{'='*40}\n"
        result += f"ğŸ“ å·²ç´¢å¼•æ–‡ä»¶æ•¸é‡: {file_count}\n"
        result += f"ğŸ“„ ç¸½ç‰‡æ®µæ•¸é‡: {total_chunks}\n"
        result += f"{'='*40}\n\n"
        result += f"ğŸ“‹ å·²ç´¢å¼•æ–‡ä»¶æ¸…å–®:\n"
        result += f"{'-'*40}\n"
        
        for idx, (source, info) in enumerate(sorted(sources_info.items()), 1):
            page_info = f", å…± {len(info['pages'])} é " if info['pages'] else ""
            result += f"{idx}. {source}\n"
            result += f"   â””â”€ {info['chunks']} å€‹ç‰‡æ®µ{page_info}\n"
        
        return result

    except Exception as e:
        return f"æŸ¥è©¢ç´¢å¼•æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"

@mcp.tool()
def query_library(query: str):
    """[æœå°‹] å¾çŸ¥è­˜åº«æœå°‹ç›¸é—œè³‡è¨Š"""
    try:
        # get_db å…§éƒ¨æœƒè™•ç† FAISS çš„ import
        db = get_db()
        if not db:
            return "çŸ¥è­˜åº«ç›®å‰æ˜¯ç©ºçš„ï¼Œè«‹å…ˆåŠ å…¥ PDF æª”æ¡ˆã€‚"

        results = db.similarity_search(query, k=4)
        
        if not results:
            return "åœ¨çŸ¥è­˜åº«ä¸­æ‰¾ä¸åˆ°ç›¸é—œè³‡è¨Šã€‚"

        response_text = f"é‡å° '{query}' çš„æœå°‹çµæœ:\n\n"
        for doc in results:
            source = doc.metadata.get('source', 'æœªçŸ¥ä¾†æº')
            page = doc.metadata.get('page', 'N/A')
            response_text += f"--- {source} (P.{page}) ---\n{doc.page_content}\n\n"
            
        return response_text
        
    except Exception as e:
        return f"æœå°‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"

if __name__ == "__main__":
    log_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), "debug_error.log")
    try:
        mcp.run()
    except Exception:
        with open(log_file, "w", encoding="utf-8") as f:
            f.write(traceback.format_exc())
        sys.exit(1)