from fastmcp import FastMCP
import os
import sys
import warnings
import traceback

# ---------------------------------------------------------
# ä¿®æ”¹é» 1: ç§»é™¤é ‚å±¤çš„ Heavy Imports
# åªä¿ç•™ os, sys, fastmcp ç­‰è¼•é‡ç´šå¥—ä»¶
# é€™æ¨£ Server å•Ÿå‹•æ™‚å°±ä¸æœƒå¡åœ¨è¼‰å…¥ PyTorch/LangChain
# ---------------------------------------------------------

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore")

mcp = FastMCP("My Local Library")

def get_base_path():
    """
    åˆ¤æ–·åŸ·è¡Œç’°å¢ƒï¼š
    å¦‚æœæ˜¯æ‰“åŒ…å¾Œçš„ exeï¼Œä½¿ç”¨åŸ·è¡Œæª”æ‰€åœ¨çš„è³‡æ–™å¤¾ (sys.executable)
    å¦‚æœæ˜¯åŸå§‹ python scriptï¼Œä½¿ç”¨æª”æ¡ˆæ‰€åœ¨çš„è³‡æ–™å¤¾ (__file__)
    """
    if getattr(sys, 'frozen', False):
        # å¦‚æœæ˜¯æ‰“åŒ…å¾Œçš„ exe
        return os.path.dirname(sys.executable)
    else:
        # å¦‚æœæ˜¯é–‹ç™¼ä¸­çš„ .py
        return os.path.dirname(os.path.abspath(__file__))

BASE_DIR = get_base_path()
DB_DIR = os.path.join(BASE_DIR, "faiss_index")

# ---------------------------------------------------------
# ä¿®æ”¹é» 2: ä½¿ç”¨å…¨åŸŸè®Šæ•¸é…åˆ Singleton æ¨¡å¼å»¶é²è¼‰å…¥
# ---------------------------------------------------------
_embedding_function = None

def get_embedding_function():
    """
    å»¶é²è¼‰å…¥ Embedding æ¨¡å‹ã€‚
    åªæœ‰åœ¨ç¬¬ä¸€æ¬¡å‘¼å«å·¥å…·æ™‚æ‰æœƒåŸ·è¡Œ import å’Œæ¨¡å‹ä¸‹è¼‰ã€‚
    """
    global _embedding_function
    if _embedding_function is None:
        # å°‡ import ç§»åˆ°é€™è£¡ï¼Œé¿å…å•Ÿå‹•æ™‚å¡ä½
        from langchain_huggingface import HuggingFaceEmbeddings
        
        print("æ­£åœ¨è¼‰å…¥ AI æ¨¡å‹ (ç¬¬ä¸€æ¬¡åŸ·è¡Œæœƒè¼ƒæ…¢)...", file=sys.stderr)
        _embedding_function = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    return _embedding_function

def get_db():
    """çµ±ä¸€å–å¾— DB å¯¦ä¾‹ (åŒ…å«å»¶é²è¼‰å…¥ FAISS)"""
    # å°‡ import ç§»åˆ°é€™è£¡
    from langchain_community.vectorstores import FAISS
    
    # ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥
    embedding_func = get_embedding_function()
    
    if os.path.exists(DB_DIR) and os.path.exists(os.path.join(DB_DIR, "index.faiss")):
        return FAISS.load_local(DB_DIR, embedding_func, allow_dangerous_deserialization=True)
    return None

def save_db(db):
    """å°‡ç´¢å¼•å­˜å›ç¡¬ç¢Ÿ"""
    db.save_local(DB_DIR)

# æ”¯æ´çš„æª”æ¡ˆæ ¼å¼
SUPPORTED_EXTENSIONS = {
    # æ–‡ä»¶é¡
    '.pdf', '.docx', '.pptx', '.xlsx', '.xls',
    # Markdown
    '.md', '.markdown',
    # ç´”æ–‡å­—
    '.txt', '.log',
    # è…³æœ¬
    '.bat', '.sh', '.ps1',
    # è¨­å®šæª”
    '.json', '.yaml', '.yml', '.ini', '.cfg', '.conf',
    # è³‡æ–™æª”
    '.csv',
    # ç¨‹å¼ç¢¼
    '.py', '.js', '.ts', '.html', '.css', '.xml'
}

# ç´”æ–‡å­—é¡å‹å‰¯æª”åï¼ˆç”¨æ–¼çµ±ä¸€è™•ç†ï¼‰
TEXT_EXTENSIONS = {
    '.txt', '.log',
    '.bat', '.sh', '.ps1',
    '.json', '.yaml', '.yml', '.ini', '.cfg', '.conf',
    '.csv',
    '.py', '.js', '.ts', '.html', '.css', '.xml'
}

# æ‰¹æ¬¡è™•ç†è¨­å®š - æ¯è™•ç† N å€‹æª”æ¡ˆå°±å¯«å…¥ä¸€æ¬¡ FAISSï¼Œé™ä½è¨˜æ†¶é«”ä½¿ç”¨ä¸¦å¢åŠ å¯é æ€§
BATCH_SIZE = 10

def get_indexed_sources(db) -> set:
    """
    å–å¾—å·²ç´¢å¼•çš„æª”æ¡ˆåç¨±é›†åˆï¼Œç”¨æ–¼é‡è¤‡æª¢æ¸¬ã€‚
    
    Returns:
        set: å·²ç´¢å¼•çš„æª”æ¡ˆåç¨±ï¼ˆsourceï¼‰é›†åˆ
    """
    if not db or not hasattr(db, 'docstore') or not hasattr(db.docstore, '_dict'):
        return set()
    return {doc.metadata.get('source') for doc in db.docstore._dict.values() if doc.metadata.get('source')}

def get_file_extension(file_path: str) -> str:
    """å–å¾—æª”æ¡ˆå‰¯æª”åï¼ˆå°å¯«ï¼‰"""
    return os.path.splitext(file_path)[1].lower()

def load_document(file_path: str):
    """
    é€šç”¨æ–‡ä»¶è¼‰å…¥å™¨ï¼šæ ¹æ“šæª”æ¡ˆé¡å‹è‡ªå‹•é¸æ“‡é©åˆçš„ loader
    
    æ”¯æ´æ ¼å¼ï¼š
    - PDF (.pdf)
    - Word (.docx)
    - PowerPoint (.pptx)
    - Excel (.xlsx, .xls)
    - Markdown (.md, .markdown)
    - ç´”æ–‡å­— (.txt, .log)
    - è…³æœ¬ (.bat, .sh, .ps1)
    - è¨­å®šæª” (.json, .yaml, .yml, .ini, .cfg, .conf)
    - è³‡æ–™æª” (.csv)
    - ç¨‹å¼ç¢¼ (.py, .js, .ts, .html, .css, .xml)
    
    Returns:
        list: Document ç‰©ä»¶åˆ—è¡¨
    """
    from langchain_core.documents import Document
    
    ext = get_file_extension(file_path)
    file_name = os.path.basename(file_path)
    
    try:
        if ext == '.pdf':
            try:
                import pypdf
            except ImportError:
                print("éŒ¯èª¤ï¼šç¼ºå°‘ pypdf æ¨¡çµ„ï¼Œè«‹åŸ·è¡Œ pip install pypdf", file=sys.stderr)
                return []
            from langchain_community.document_loaders import PyPDFLoader
            loader = PyPDFLoader(file_path)
            return loader.load()
        
        elif ext == '.docx':
            import docx2txt
            text = docx2txt.process(file_path)
            if text.strip():
                return [Document(
                    page_content=text,
                    metadata={"source": file_name, "file_type": "docx"}
                )]
            return []
        
        elif ext == '.pptx':
            from pptx import Presentation
            prs = Presentation(file_path)
            documents = []
            for slide_num, slide in enumerate(prs.slides, 1):
                slide_text = []
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text.strip():
                        slide_text.append(shape.text)
                if slide_text:
                    documents.append(Document(
                        page_content="\n".join(slide_text),
                        metadata={"source": file_name, "page": slide_num, "file_type": "pptx"}
                    ))
            return documents
        
        elif ext in ['.xlsx', '.xls']:
            import openpyxl
            wb = openpyxl.load_workbook(file_path, data_only=True)
            documents = []
            for sheet_name in wb.sheetnames:
                sheet = wb[sheet_name]
                rows_text = []
                for row in sheet.iter_rows(max_row=1000):  # é™åˆ¶æœ€å¤§è¡Œæ•¸é¿å…è¨˜æ†¶é«”å•é¡Œ
                    row_values = [str(cell.value) if cell.value is not None else "" for cell in row]
                    if any(v.strip() for v in row_values):
                        rows_text.append(" | ".join(row_values))
                if rows_text:
                    documents.append(Document(
                        page_content="\n".join(rows_text),
                        metadata={"source": file_name, "sheet": sheet_name, "file_type": "excel"}
                    ))
            return documents
        
        elif ext in ['.md', '.markdown']:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
            if text.strip():
                return [Document(
                    page_content=text,
                    metadata={"source": file_name, "file_type": "markdown"}
                )]
            return []
        
        elif ext in TEXT_EXTENSIONS:
            # ä½¿ç”¨ chardet è‡ªå‹•åµæ¸¬ç·¨ç¢¼
            import chardet
            
            try:
                with open(file_path, 'rb') as f:
                    raw_data = f.read()
                
                # åµæ¸¬ç·¨ç¢¼
                detected = chardet.detect(raw_data)
                encoding = detected.get('encoding', 'utf-8') or 'utf-8'
                confidence = detected.get('confidence', 0)
                
                # è§£ç¢¼æ–‡å­—
                text = raw_data.decode(encoding, errors='ignore')
                
                if text.strip():
                    # æ ¹æ“šå‰¯æª”åæ±ºå®š file_type
                    file_type_map = {
                        '.txt': 'text', '.log': 'log',
                        '.bat': 'script', '.sh': 'script', '.ps1': 'script',
                        '.json': 'config', '.yaml': 'config', '.yml': 'config',
                        '.ini': 'config', '.cfg': 'config', '.conf': 'config',
                        '.csv': 'data',
                        '.py': 'code', '.js': 'code', '.ts': 'code',
                        '.html': 'code', '.css': 'code', '.xml': 'code'
                    }
                    file_type = file_type_map.get(ext, 'text')
                    
                    return [Document(
                        page_content=text,
                        metadata={
                            "source": file_name,
                            "file_type": file_type,
                            "encoding": encoding,
                            "encoding_confidence": round(confidence, 2)
                        }
                    )]
            except Exception as e:
                print(f"è®€å–ç´”æ–‡å­—æª”æ¡ˆ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", file=sys.stderr)
            return []
        
        else:
            return []
    
    except Exception as e:
        print(f"è¼‰å…¥æ–‡ä»¶ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", file=sys.stderr)
        return []

# ---------------------------------------------------------
# ä¿®æ”¹é» 3: å°‡å·¥å…·æ‰€éœ€çš„ Import ç§»è‡³å‡½å¼å…§éƒ¨
# ---------------------------------------------------------

@mcp.tool()
def add_folder_to_library(folder_path: str):
    """[æ‰¹æ¬¡è™•ç†] è®€å–è³‡æ–™å¤¾å…§æ‰€æœ‰æ”¯æ´çš„æ–‡ä»¶ä¸¦åŠ å…¥çŸ¥è­˜åº«
    
    æ”¯æ´æ ¼å¼ï¼šPDF, DOCX, PPTX, XLSX, XLS, MD, TXT, LOG, BAT, SH, PS1, JSON, YAML, YML, INI, CFG, CONF, CSV, PY, JS, TS, HTML, CSS, XML
    
    å„ªåŒ–åŠŸèƒ½ï¼š
    - åˆ†æ‰¹å¯«å…¥ FAISSï¼ˆæ¯ N å€‹æª”æ¡ˆå¯«å…¥ä¸€æ¬¡ï¼Œé™ä½è¨˜æ†¶é«”ä½¿ç”¨ï¼‰
    - è‡ªå‹•è·³éå·²ç´¢å¼•çš„æª”æ¡ˆï¼ˆé‡è¤‡æª¢æ¸¬ï¼‰
    - é€²åº¦å›å ±ï¼ˆåœ¨ stderr è¼¸å‡ºè™•ç†é€²åº¦ï¼‰
    - æ–·é»çºŒå‚³å‹å–„ï¼ˆåˆ†æ‰¹å¯«å…¥ï¼Œä¸­é€”å¤±æ•—ä¹Ÿä¿ç•™éƒ¨åˆ†æˆæœï¼‰
    """
    # Import moved locally
    import glob
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS

    folder_path = folder_path.strip('"').strip("'")
    
    if not os.path.exists(folder_path):
        return f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è³‡æ–™å¤¾ -> {folder_path}"

    # æœå°‹æ‰€æœ‰æ”¯æ´çš„æª”æ¡ˆæ ¼å¼
    all_files = []
    for ext in SUPPORTED_EXTENSIONS:
        all_files.extend(glob.glob(os.path.join(folder_path, f"*{ext}")))
        # ä¹Ÿæœå°‹å¤§å¯«å‰¯æª”å
        all_files.extend(glob.glob(os.path.join(folder_path, f"*{ext.upper()}")))
    
    # å»é‡è¤‡ä¸¦æ’åºï¼ˆç¢ºä¿è™•ç†é †åºä¸€è‡´ï¼‰
    all_files = sorted(set(all_files))
    
    if not all_files:
        supported_list = ", ".join(SUPPORTED_EXTENSIONS)
        return f"åœ¨ '{folder_path}' ä¸­æ‰¾ä¸åˆ°ä»»ä½•æ”¯æ´çš„æª”æ¡ˆã€‚\næ”¯æ´æ ¼å¼: {supported_list}"

    # å–å¾—å·²ç´¢å¼•çš„æª”æ¡ˆï¼Œç”¨æ–¼é‡è¤‡æª¢æ¸¬
    current_db = get_db()
    indexed_sources = get_indexed_sources(current_db)
    
    # éæ¿¾æ‰å·²ç´¢å¼•çš„æª”æ¡ˆ
    files_to_process = []
    skipped_files = []
    for file_path in all_files:
        file_name = os.path.basename(file_path)
        if file_name in indexed_sources:
            skipped_files.append(file_name)
        else:
            files_to_process.append(file_path)
    
    if not files_to_process:
        if skipped_files:
            return f"ğŸ“‹ è³‡æ–™å¤¾ä¸­çš„ {len(skipped_files)} å€‹æª”æ¡ˆéƒ½å·²åœ¨çŸ¥è­˜åº«ä¸­ï¼Œç„¡éœ€é‡è¤‡ç´¢å¼•ã€‚"
        return "æ²’æœ‰æ‰¾åˆ°éœ€è¦è™•ç†çš„æª”æ¡ˆã€‚"

    # é€²åº¦å›å ±
    total_files = len(files_to_process)
    print(f"[é–‹å§‹è™•ç†] å…± {total_files} å€‹æ–°æª”æ¡ˆå¾…è™•ç†ï¼Œå·²è·³é {len(skipped_files)} å€‹é‡è¤‡æª”æ¡ˆ", file=sys.stderr)
    
    processed_files = []
    failed_files = []
    total_splits_count = 0
    batch_count = 0
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)
    embedding_func = get_embedding_function()
    
    batch_splits = []  # ç•¶å‰æ‰¹æ¬¡çš„ç‰‡æ®µ

    for idx, file_path in enumerate(files_to_process, 1):
        file_name = os.path.basename(file_path)
        
        # é€²åº¦å›å ±
        print(f"[{idx}/{total_files}] è™•ç†ä¸­: {file_name}", file=sys.stderr)
        
        try:
            docs = load_document(file_path)
            if docs:
                splits = text_splitter.split_documents(docs)
                if splits:
                    for split in splits:
                        split.metadata["source"] = file_name
                    batch_splits.extend(splits)
                    processed_files.append(file_name)
                else:
                    failed_files.append((file_name, "æ–‡ä»¶å…§å®¹ç‚ºç©º"))
            else:
                failed_files.append((file_name, "ç„¡æ³•è®€å–å…§å®¹"))
        except Exception as e:
            failed_files.append((file_name, str(e)))
            print(f"[éŒ¯èª¤] {file_name}: {e}", file=sys.stderr)
        
        # åˆ†æ‰¹å¯«å…¥ FAISS - æ¯ BATCH_SIZE å€‹æª”æ¡ˆæˆ–æœ€å¾Œä¸€å€‹æª”æ¡ˆæ™‚å¯«å…¥
        if len(processed_files) > 0 and (len(processed_files) % BATCH_SIZE == 0 or idx == total_files):
            if batch_splits:
                try:
                    batch_count += 1
                    print(f"[å¯«å…¥æ‰¹æ¬¡ {batch_count}] æ­£åœ¨å¯«å…¥ {len(batch_splits)} å€‹ç‰‡æ®µåˆ° FAISS...", file=sys.stderr)
                    
                    # é‡æ–°å–å¾—æœ€æ–°çš„ DBï¼ˆå¯èƒ½åœ¨ä¸Šä¸€æ‰¹æ¬¡å·²æ›´æ–°ï¼‰
                    current_db = get_db()
                    
                    if current_db:
                        current_db.add_documents(batch_splits)
                        save_db(current_db)
                    else:
                        new_db = FAISS.from_documents(batch_splits, embedding_func)
                        save_db(new_db)
                    
                    total_splits_count += len(batch_splits)
                    batch_splits = []  # æ¸…ç©ºæ‰¹æ¬¡ï¼Œæº–å‚™ä¸‹ä¸€æ‰¹
                    print(f"[å¯«å…¥æ‰¹æ¬¡ {batch_count}] å®Œæˆï¼ç´¯è¨ˆå·²å¯«å…¥ {total_splits_count} å€‹ç‰‡æ®µ", file=sys.stderr)
                    
                except Exception as e:
                    error_msg = f"å¯«å…¥æ‰¹æ¬¡ {batch_count} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
                    print(f"[åš´é‡éŒ¯èª¤] {error_msg}", file=sys.stderr)
                    # è¨˜éŒ„é€™æ‰¹æª”æ¡ˆç‚ºå¤±æ•—ï¼ˆä½†ä¿ç•™ä¹‹å‰æ‰¹æ¬¡çš„æˆæœï¼‰
                    return f"âš ï¸ éƒ¨åˆ†è™•ç†å®Œæˆï¼Œä½†åœ¨æ‰¹æ¬¡ {batch_count} æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚\n" \
                           f"ğŸ“ å·²æˆåŠŸè™•ç†: {len(processed_files) - len(batch_splits)} å€‹æª”æ¡ˆ\n" \
                           f"ğŸ“„ å·²å¯«å…¥: {total_splits_count} å€‹ç‰‡æ®µ\n" \
                           f"âŒ éŒ¯èª¤: {error_msg}"

    # çµ„åˆæœ€çµ‚çµæœ
    result = f"âœ… æ‰¹æ¬¡è™•ç†å®Œæˆï¼\n"
    result += f"{'='*40}\n"
    result += f"ğŸ“ å…±è™•ç† {len(processed_files)} å€‹æª”æ¡ˆ\n"
    result += f"ğŸ“„ æ–°å¢ {total_splits_count} å€‹ç‰‡æ®µ\n"
    result += f"ğŸ“¦ åˆ† {batch_count} å€‹æ‰¹æ¬¡å¯«å…¥\n"
    
    if skipped_files:
        result += f"\nâ­ï¸ å·²è·³é {len(skipped_files)} å€‹é‡è¤‡æª”æ¡ˆï¼ˆå·²å­˜åœ¨æ–¼çŸ¥è­˜åº«ï¼‰\n"
    
    if failed_files:
        result += f"\nâš ï¸ ä»¥ä¸‹ {len(failed_files)} å€‹æª”æ¡ˆè™•ç†å¤±æ•—:\n"
        for file_name, reason in failed_files:
            result += f"  - {file_name}: {reason}\n"
    
    return result

@mcp.tool()
def add_pdf_to_library(pdf_path: str):
    """[å–®æª”è™•ç†] å°‡ PDF åŠ å…¥çŸ¥è­˜åº«ï¼ˆå‘ä¸‹ç›¸å®¹ï¼Œå»ºè­°ä½¿ç”¨ add_document_to_libraryï¼‰"""
    return add_document_to_library(pdf_path)

@mcp.tool()
def add_document_to_library(file_path: str):
    """[å–®æª”è™•ç†] å°‡æ–‡ä»¶åŠ å…¥çŸ¥è­˜åº«
    
    æ”¯æ´æ ¼å¼ï¼šPDF, DOCX, PPTX, XLSX, XLS, MD, TXT, LOG, BAT, SH, PS1, JSON, YAML, YML, INI, CFG, CONF, CSV, PY, JS, TS, HTML, CSS, XML
    """
    # Import moved locally
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import FAISS

    file_path = file_path.strip('"').strip("'")
    
    if not os.path.exists(file_path):
        return f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ -> {file_path}"
    
    # æª¢æŸ¥æª”æ¡ˆæ ¼å¼
    ext = get_file_extension(file_path)
    if ext not in SUPPORTED_EXTENSIONS:
        supported_list = ", ".join(SUPPORTED_EXTENSIONS)
        return f"éŒ¯èª¤ï¼šä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ '{ext}'\næ”¯æ´æ ¼å¼: {supported_list}"

    try:
        docs = load_document(file_path)
        
        if not docs:
            return f"æ–‡ä»¶å…§å®¹ç‚ºç©ºæˆ–ç„¡æ³•è®€å–: {os.path.basename(file_path)}"
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)
        splits = text_splitter.split_documents(docs)
        
        if not splits:
            return "æ–‡ä»¶å…§å®¹ç‚ºç©ºæˆ–ç„¡æ³•è®€å–ã€‚"

        # ç¢ºä¿ source metadata æ­£ç¢º
        for split in splits:
            split.metadata["source"] = os.path.basename(file_path)

        # å¯«å…¥ FAISS
        current_db = get_db()
        embedding_func = get_embedding_function()

        if current_db:
            current_db.add_documents(splits)
            save_db(current_db)
        else:
            new_db = FAISS.from_documents(splits, embedding_func)
            save_db(new_db)
        
        file_type_emoji = {
            '.pdf': 'ğŸ“•',
            '.docx': 'ğŸ“˜',
            '.pptx': 'ğŸ“™',
            '.xlsx': 'ğŸ“—',
            '.xls': 'ğŸ“—',
            '.md': 'ğŸ“',
            '.markdown': 'ğŸ“',
            # ç´”æ–‡å­—
            '.txt': 'ğŸ“„',
            '.log': 'ğŸ“‹',
            # è…³æœ¬
            '.bat': 'âš™ï¸',
            '.sh': 'âš™ï¸',
            '.ps1': 'âš™ï¸',
            # è¨­å®šæª”
            '.json': 'ğŸ”§',
            '.yaml': 'ğŸ”§',
            '.yml': 'ğŸ”§',
            '.ini': 'ğŸ”§',
            '.cfg': 'ğŸ”§',
            '.conf': 'ğŸ”§',
            # è³‡æ–™æª”
            '.csv': 'ğŸ“Š',
            # ç¨‹å¼ç¢¼
            '.py': 'ğŸ',
            '.js': 'ğŸ’»',
            '.ts': 'ğŸ’»',
            '.html': 'ğŸŒ',
            '.css': 'ğŸ¨',
            '.xml': 'ğŸ“°'
        }
        emoji = file_type_emoji.get(ext, 'ğŸ“„')
        
        return f"âœ… æˆåŠŸï¼{emoji} å·²å°‡ '{os.path.basename(file_path)}' çš„ {len(splits)} å€‹ç‰‡æ®µåŠ å…¥çŸ¥è­˜åº«ã€‚"
    
    except Exception as e:
        return f"è™•ç†æ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"

@mcp.tool()
def list_indexed_files():
    """[æŸ¥è©¢] åˆ—å‡ºçŸ¥è­˜åº«ä¸­å·²ç´¢å¼•çš„æ‰€æœ‰æ–‡ä»¶åŠçµ±è¨ˆè³‡è¨Š"""
    try:
        db = get_db()
        if not db:
            return "ğŸ“­ çŸ¥è­˜åº«ç›®å‰æ˜¯ç©ºçš„ï¼Œè«‹å…ˆä½¿ç”¨ add_pdf_to_library æˆ– add_folder_to_library åŠ å…¥æ–‡ä»¶ã€‚"

        # å¾ docstore ä¸­æå–æ‰€æœ‰æ–‡ä»¶çš„ metadata
        sources_info = {}  # source -> {'pages': set(), 'chunks': count}
        total_chunks = 0
        
        for doc_id in db.docstore._dict:
            doc = db.docstore._dict[doc_id]
            source = doc.metadata.get('source', 'æœªçŸ¥ä¾†æº')
            page = doc.metadata.get('page', None)
            
            if source not in sources_info:
                sources_info[source] = {'pages': set(), 'chunks': 0}
            
            sources_info[source]['chunks'] += 1
            if page is not None:
                sources_info[source]['pages'].add(page)
            total_chunks += 1

        # çµ„åˆè¼¸å‡ºçµæœ
        file_count = len(sources_info)
        result = f"ğŸ“š çŸ¥è­˜åº«çµ±è¨ˆè³‡è¨Š\n"
        result += f"{'='*40}\n"
        result += f"ğŸ“ å·²ç´¢å¼•æ–‡ä»¶æ•¸é‡: {file_count}\n"
        result += f"ğŸ“„ ç¸½ç‰‡æ®µæ•¸é‡: {total_chunks}\n"
        result += f"{'='*40}\n\n"
        result += f"ğŸ“‹ å·²ç´¢å¼•æ–‡ä»¶æ¸…å–®:\n"
        result += f"{'-'*40}\n"
        
        for idx, (source, info) in enumerate(sorted(sources_info.items()), 1):
            page_info = f", å…± {len(info['pages'])} é " if info['pages'] else ""
            result += f"{idx}. {source}\n"
            result += f"   â””â”€ {info['chunks']} å€‹ç‰‡æ®µ{page_info}\n"
        
        return result

    except Exception as e:
        return f"æŸ¥è©¢ç´¢å¼•æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"

@mcp.tool()
def query_library(query: str):
    """[æœå°‹] å¾çŸ¥è­˜åº«æœå°‹ç›¸é—œè³‡è¨Š"""
    try:
        # get_db å…§éƒ¨æœƒè™•ç† FAISS çš„ import
        db = get_db()
        if not db:
            return "çŸ¥è­˜åº«ç›®å‰æ˜¯ç©ºçš„ï¼Œè«‹å…ˆåŠ å…¥ PDF æª”æ¡ˆã€‚"

        results = db.similarity_search(query, k=4)
        
        if not results:
            return "åœ¨çŸ¥è­˜åº«ä¸­æ‰¾ä¸åˆ°ç›¸é—œè³‡è¨Šã€‚"

        response_text = f"é‡å° '{query}' çš„æœå°‹çµæœ:\n\n"
        for doc in results:
            source = doc.metadata.get('source', 'æœªçŸ¥ä¾†æº')
            page = doc.metadata.get('page', 'N/A')
            response_text += f"--- {source} (P.{page}) ---\n{doc.page_content}\n\n"
            
        return response_text
        
    except Exception as e:
        return f"æœå°‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"

if __name__ == "__main__":
    log_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), "debug_error.log")
    try:
        mcp.run()
    except Exception:
        with open(log_file, "w", encoding="utf-8") as f:
            f.write(traceback.format_exc())
        sys.exit(1)